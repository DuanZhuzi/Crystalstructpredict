{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37749f1-d9d1-4c47-a646-292d14103f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2099e132-97b3-449d-916f-cb8a48ec0d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b1a5c60-c3cd-4abd-afa6-a97df2107572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6255844-414e-48b3-a160-85348f8866ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract features\n",
    "elements = set([el for row in df['element_symbols'].str.split(',') for el in row])\n",
    "data = []\n",
    "for i, row in df.iterrows():\n",
    "    element_counts = Counter(row['element_symbols'])\n",
    "    feature = [element_counts[el] / row['nsites'] for el in elements]\n",
    "    feature.extend([1 if el in row['element_symbols'] else 0 for el in elements])\n",
    "    data.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e788daf8-f054-4da0-a4f3-3d39d8b094c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array(data)\n",
    "y = df['spacegroup_num'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ed9f23c-adb7-4483-a40a-aacf7c509986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e6d7043-87f0-4188-8709-6e61647c3802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10628, 472)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbc51ebc-2cbc-4ca1-a684-03086b196461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8502, 472)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64af1ea8-31f4-4889-9bcc-324aee78f691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8502,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe5d69e9-f125-41a0-91b9-e97fe124ceb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_network(layer_sizes):\n",
    "    \n",
    "    def init(key, scale=1e-2):\n",
    "        params = []\n",
    "        for n_in, n_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            weight_key, bias_key = jax.random.split(key)\n",
    "            weight = scale * jax.random.normal(weight_key, (n_in, n_out))\n",
    "            bias = scale * jax.random.normal(bias_key, (n_out,))\n",
    "            params.append((weight, bias))\n",
    "        return params\n",
    "\n",
    "    def relu(x):\n",
    "        return jnp.maximum(0, x)\n",
    "\n",
    "    def apply(params, x):\n",
    "        for w, b in params[:-1]:\n",
    "            x = relu(jnp.dot(x, w) + b)\n",
    "        final_w, final_b = params[-1]\n",
    "        return jnp.dot(x, final_w) + final_b\n",
    "\n",
    "    return init, apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5276e965-618d-46af-955c-7606d2237c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_sizes = [472, 220, 128, 64, 1] \n",
    "init_fn, apply_fn = make_network(layer_sizes)\n",
    "key = jax.random.PRNGKey(42)\n",
    "params = init_fn(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b8ff34b-02f6-405b-8f3c-e6e6a3a38b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140669"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.flatten_util import ravel_pytree\n",
    "ravel_pytree(params)[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e8a3441-a425-4aca-a175-6b26c1383525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_entropy(params, x, y):\n",
    "    logits = apply_fn(params, x)\n",
    "    return jnp.sum(y * jax.nn.log_softmax(logits))\n",
    "\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    return -jnp.mean(jax.vmap(cross_entropy, (None, 0, 0),0)(params, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0b6d9f59-5fa7-4e86-b295-8a3f7a1d906c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(params, x, y):\n",
    "    \"\"\" Mean squared error loss function \"\"\"\n",
    "    preds = apply_fn(params, x)\n",
    "    return jnp.mean((preds - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "583428cc-f999-4d23-947b-c2945a0e797b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the update function\n",
    "@jax.jit\n",
    "def update(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss)(params, x, y)\n",
    "    return jax.tree_map(lambda p, g: p - learning_rate * g, params, grads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7d9188ed-f940-440e-9a82-ef3f1201f41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the accuracy function\n",
    "def accuracy(params, x, y):\n",
    "    predictions = apply_fn(params, x)\n",
    "    return jnp.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d782ec3-bdc4-419d-925d-1d741f1f354f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ArrayImpl' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m accuracy(params, x_train, y_train)\n\u001b[1;32m     28\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m accuracy(params, x_test, y_test)\n\u001b[0;32m---> 29\u001b[0m lossf \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.10f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lossf = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.10f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,test accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.10f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ArrayImpl' object is not callable"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "num_complete_batches, leftover = divmod(train_size, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    key, subkey = jax.random.split(key)\n",
    "    permutation = jax.random.permutation(subkey, train_size)\n",
    "    x_train = x_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        # Get batch data\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        x_batch = x_train[batch_start:batch_end]\n",
    "        y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "        # Update parameters\n",
    "        params = update(params, x_batch, y_batch, learning_rate)\n",
    "\n",
    "    # Compute accuracy on training and test sets\n",
    "    train_accuracy = accuracy(params, x_train, y_train)\n",
    "    test_accuracy = accuracy(params, x_test, y_test)\n",
    "    lossf = loss(params, x_train, y_train)\n",
    "    print(f\"Epoch {epoch}: train accuracy = {train_accuracy:.10f}, lossf = {loss:.10f},test accuracy = {test_accuracy:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "26709fb9-b8e5-4fb0-bc12-fec697de772f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd41622c-5352-4e91-9eea-71211f351db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "def init_params(layer_sizes, key):\n",
    "    \"\"\" Initialize the parameters of the MLP \"\"\"\n",
    "    params = []\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        key, subkey = random.split(key)\n",
    "        # Use He initialization for the weights\n",
    "        input_size, output_size = layer_sizes[i-1], layer_sizes[i]\n",
    "        W = random.normal(subkey, (input_size, output_size)) * jnp.sqrt(2 / input_size)\n",
    "        b = jnp.zeros((output_size,))\n",
    "        params.append((W, b))\n",
    "    return params\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\" ReLU activation function \"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, inputs):\n",
    "    \"\"\" MLP prediction function \"\"\"\n",
    "    for W, b in params:\n",
    "        outputs = jnp.dot(inputs, W) + b\n",
    "        inputs = relu(outputs)\n",
    "    return outputs\n",
    "\n",
    "def loss(params, inputs, targets):\n",
    "    \"\"\" Mean squared error loss function \"\"\"\n",
    "    preds = predict(params, inputs)\n",
    "    return jnp.mean((preds - targets)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e684193-ac43-4a2a-b76d-4fe60bfd0b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, x, y, learning_rate):\n",
    "    \"\"\" Update step for the MLP parameters \"\"\"\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(W - learning_rate*dW, b - learning_rate*db)\n",
    "            for (W, b), (dW, db) in zip(params, grads)]\n",
    "\n",
    "@jit\n",
    "def train_mlp(train_X, train_Y, test_X, test_Y, layer_sizes, learning_rate, num_epochs):\n",
    "    \"\"\" Train an MLP and evaluate its performance on train and test data \"\"\"\n",
    "    key = random.PRNGKey(42)\n",
    "    params = init_params(layer_sizes, key)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in zip(train_X, train_Y):\n",
    "            params = update(params, x, y, learning_rate)\n",
    "\n",
    "        train_loss = loss(params, train_X, train_Y)\n",
    "        test_loss = loss(params, test_X, test_Y)\n",
    "\n",
    "        print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch+1, train_loss, test_loss))\n",
    "\n",
    "    # Compute accuracy on the test set\n",
    "    preds = predict(params, test_X)\n",
    "    accuracy = jnp.mean(jnp.round(preds) == test_Y)\n",
    "    print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bc8a3058-e730-48a0-a31a-fe2797cab5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function train_mlp at /tmp/ipykernel_10423/3470114896.py:8 for jit. This concrete value was not available in Python because it depends on the value of the argument layer_sizes[0].\nThe error occurred while tracing the function train_mlp at /tmp/ipykernel_10423/3470114896.py:8 for jit. This concrete value was not available in Python because it depends on the value of the argument layer_sizes[1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[89], line 12\u001b[0m, in \u001b[0;36mtrain_mlp\u001b[0;34m(train_X, train_Y, test_X, test_Y, layer_sizes, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Train an MLP and evaluate its performance on train and test data \"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43minit_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_X, train_Y):\n",
      "Cell \u001b[0;32mIn[85], line 9\u001b[0m, in \u001b[0;36minit_params\u001b[0;34m(layer_sizes, key)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use He initialization for the weights\u001b[39;00m\n\u001b[1;32m      8\u001b[0m input_size, output_size \u001b[38;5;241m=\u001b[39m layer_sizes[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], layer_sizes[i]\n\u001b[0;32m----> 9\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m input_size)\n\u001b[1;32m     10\u001b[0m b \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mzeros((output_size,))\n\u001b[1;32m     11\u001b[0m params\u001b[38;5;241m.\u001b[39mappend((W, b))\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.10/site-packages/jax/_src/random.py:578\u001b[0m, in \u001b[0;36mnormal\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype argument to `normal` must be a float or complex dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[0;32m--> 578\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_named_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _normal(key, shape, dtype)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.10/site-packages/jax/_src/core.py:2039\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   2038\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 2039\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function train_mlp at /tmp/ipykernel_10423/3470114896.py:8 for jit. This concrete value was not available in Python because it depends on the value of the argument layer_sizes[0].\nThe error occurred while tracing the function train_mlp at /tmp/ipykernel_10423/3470114896.py:8 for jit. This concrete value was not available in Python because it depends on the value of the argument layer_sizes[1]."
     ]
    }
   ],
   "source": [
    "# Train the MLP model\n",
    "layer_sizes = [472, 220, 128, 64, 1] # Input size, hidden layer sizes, output size\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "train_mlp(x_train, y_train, x_test, y_test, layer_sizes, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8d08e-70c3-4b7a-96b3-d5967076589e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
