{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d105d1ef-e0f4-4a11-aa65-6efb8d9dae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4792a6-c7c2-4b2d-90f2-74cbee732b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "class Transformer(hk.Module):\n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        # Define the layers of the Transformer\n",
    "        self.enc_layers = [\n",
    "            hk.transform(\n",
    "                lambda x: self._encoder_layer(x, self.num_heads, self.d_model, self.d_ff)\n",
    "            )\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Encoder input is the same as the decoder output\n",
    "        encoder_input = inputs\n",
    "        # Run the encoder layers\n",
    "        for enc_layer in self.enc_layers:\n",
    "            encoder_input = enc_layer(encoder_input)\n",
    "        return encoder_input\n",
    "\n",
    "    def _encoder_layer(self, inputs, num_heads, d_model, d_ff):\n",
    "        # Multi-head self-attention layer\n",
    "        x = hk.MultiHeadAttention(\n",
    "            key_size=d_model // num_heads,\n",
    "            value_size=d_model // num_heads,\n",
    "            num_heads=num_heads,\n",
    "            w_init=hk.initializers.TruncatedNormal(stddev=1.0 / jnp.sqrt(d_model)),\n",
    "        )(inputs, inputs)\n",
    "        # Add and normalize\n",
    "        x = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x + inputs)\n",
    "        # Feedforward layer\n",
    "        x = hk.Linear(d_ff)(x)\n",
    "        x = jax.nn.gelu(x)\n",
    "        # Add and normalize\n",
    "        x = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)(x + inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28a6c4a-a930-46d7-88fa-20ab2cdf672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss_fn(model, inputs, targets):\n",
    "    # Run the model on the inputs\n",
    "    logits = model(inputs)\n",
    "    # Compute the cross-entropy loss\n",
    "    ce_loss = jnp.mean(optax.softmax_cross_entropy(logits, targets))\n",
    "    # Return the average loss across the batch\n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a7948b-3934-40b2-9f4f-9374b9e5ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training step\n",
    "@jax.jit\n",
    "def train_step(model, optimizer, inputs, targets):\n",
    "    # Compute the gradients of the loss with respect to the model parameters\n",
    "    grads = jax.grad(loss_fn)(model, inputs, targets)\n",
    "    # Update the model parameters using the optimizer\n",
    "    updates, optimizer_state = optimizer.update(grads)\n",
    "    model = optax.apply_updates(model, updates)\n",
    "    # Return the updated model and optimizer state\n",
    "    return model, optimizer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2984a6d-1bd1-4daa-b674-4f5bd2ce3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data\n",
    "with open('Y.txt', 'r') as f:\n",
    "    Y = f.read().splitlines()\n",
    "with open('X.txt', 'r') as f:\n",
    "    X = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be88ebd3-2f8a-49a0-9c5d-eb176ed456ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb21e857-00b6-4a81-bf31-c2423da55fc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `hk.Module`s must be initialized inside an `hk.transform`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the Transformer model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_ff\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.10/site-packages/haiku/_src/module.py:122\u001b[0m, in \u001b[0;36mModuleMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Now attempt to initialize the object.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m init \u001b[38;5;241m=\u001b[39m wrap_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (config\u001b[38;5;241m.\u001b[39mget_config()\u001b[38;5;241m.\u001b[39mmodule_auto_repr \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUTO_REPR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m    126\u001b[0m   module\u001b[38;5;241m.\u001b[39m_auto_repr \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mauto_repr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ldm/lib/python3.10/site-packages/haiku/_src/module.py:403\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the original method with a group name set before and after.\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m base\u001b[38;5;241m.\u001b[39mframe_stack:\n\u001b[0;32m--> 403\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `hk.Module`s must be initialized inside an `hk.transform`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# Submodules are associated with this method. We allow users to associate\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# submodules with a different method than the one being called via\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# `@name_like(\"other_method\")`. Interceptors and custom getters are still\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# provided the actual method name (e.g. \"submodule_method_name\" is only used\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# for naming submodules).\u001b[39;00m\n\u001b[1;32m    411\u001b[0m submodule_method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(unbound_method, _CUSTOM_NAME, method_name)\n",
      "\u001b[0;31mValueError\u001b[0m: All `hk.Module`s must be initialized inside an `hk.transform`."
     ]
    }
   ],
   "source": [
    "# Create the Transformer model\n",
    "model = Transformer(num_layers=num_layers, num_heads=num_heads, d_model=d_model, d_ff=d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d9847-b1d4-4c73-b504-55e6163cbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = optax.adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900997a-18a8-4850-9cc4-37c4a2fb646d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data\n",
    "    perm = jax.random.permutation(jax.random.PRNGKey(0), len(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
